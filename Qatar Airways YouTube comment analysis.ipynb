{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e3d390",
   "metadata": {},
   "source": [
    "##### 1. Scrape all the playlist id from the channel and turn them into a dataframe called 'playlist_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import html.parser as htmlparser\n",
    "import html\n",
    "import time\n",
    "import datetime as dt\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"Your google developer key\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request2 = youtube.playlists().list(\n",
    "        fields = 'items/snippet,items/id',\n",
    "        channelId = 'UCi8xUU_lg3zr8UcBXLdEheQ',\n",
    "        maxResults = 50,\n",
    "        part = 'snippet'\n",
    ")\n",
    "\n",
    "platlists = []\n",
    "\n",
    "response2 = request2.execute()\n",
    "\n",
    "for item2 in response2['items']:\n",
    "    playlist = item2['snippet']\n",
    "    playlist_id=item2['id']\n",
    "    platlists.append([\n",
    "    playlist_id,    \n",
    "    playlist['title']])\n",
    "    \n",
    "playlist_id = pd.DataFrame(platlists, columns=['playlist_id','playlist_title'])\n",
    "playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b960db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all playlist id into a list.\n",
    "play_list=playlist_id.playlist_id.to_list()\n",
    "play_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b45a8",
   "metadata": {},
   "source": [
    "##### 2. Scrape every video id from all the playlist and turn them into a dataframe called 'video_id_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a28dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_data = pd.DataFrame() \n",
    "for i in play_list:\n",
    "    try:\n",
    "        request3 = youtube.playlistItems().list(\n",
    "                fields = 'nextPageToken,items/snippet/resourceId,items/snippet/title',\n",
    "                playlistId = i,\n",
    "                maxResults = 50,\n",
    "                part = 'snippet'\n",
    "        )\n",
    "\n",
    "        videos = []\n",
    "\n",
    "\n",
    "        response3 = request3.execute()\n",
    "\n",
    "        for item3 in response3['items']:\n",
    "            video = item3['snippet']\n",
    "            videos.append([str(i),\n",
    "            video['resourceId']['videoId'],    \n",
    "            video['title']])\n",
    "\n",
    "        video_id = pd.DataFrame(videos, columns=['playlist_id','videoId','video_Title'])\n",
    "        video_id_data = pd.concat([video_id_data, video_id], ignore_index=True)\n",
    "    except:\n",
    "        print(f'fail to scrape comment from playlist_id:{i}')\n",
    "        time.sleep(5)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1316918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save every video id into a list\n",
    "video_list=video_id_data.videoId.to_list()\n",
    "video_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69858fed",
   "metadata": {},
   "source": [
    "##### 3. Scrape every top-level comment from every video id and turned into into dataframe called 'comment_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "comment_data = pd.DataFrame() \n",
    "\n",
    "for i in video_list:\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "        part = 'snippet,replies', \n",
    "        videoId = i\n",
    "    )\n",
    "\n",
    "        comments = []\n",
    "\n",
    "        while request:\n",
    "\n",
    "            response = request.execute()\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append([\n",
    "                comment['videoId'],    \n",
    "                comment['authorDisplayName'],\n",
    "                comment['publishedAt'],\n",
    "                comment['updatedAt'],\n",
    "                comment['likeCount'],\n",
    "                html.unescape(comment['textDisplay']),\n",
    "                item['snippet']['totalReplyCount']\n",
    "                            ])\n",
    "                request = youtube.commentThreads().list_next(\n",
    "                request, response)\n",
    "                youtubedata_data = pd.DataFrame(comments, columns=['videoId','author', 'published_at', 'updated_at', 'like_count', 'comment','Reply_count'])   \n",
    "        comment_data = pd.concat([comment_data, youtubedata_data], ignore_index=True)\n",
    "    except:\n",
    "        print(f'fail to scrape comment from video_id:{i}')\n",
    "        time.sleep(5)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604aec3",
   "metadata": {},
   "source": [
    "##### 4. Perform data cleaning to keep hashtag only instead of html embeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(comment_data1['comment'].count()):\n",
    "    st=comment_data1.loc[i,'comment']\n",
    "    comment_data1.loc[i,'comment']=re.sub('https.{0,500}','',st)\n",
    "    comment_data1.loc[i,'comment']=re.sub('<br>','',st)\n",
    "comment_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d502a7",
   "metadata": {},
   "source": [
    "##### 5. Check if there's any comment containing empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13040ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data1=comment_data1[(comment_data1.comment == '')].reset_index(drop=True)\n",
    "comment_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af68ca",
   "metadata": {},
   "source": [
    "##### 6. Merging various dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data1=comment_data1.merge(video_id_data, how='inner', on='videoId')\n",
    "comment_data1=comment_data1.merge(playlist_id, how='inner', on='playlist_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e8fb5",
   "metadata": {},
   "source": [
    "##### 7. Since some comments are not in English, I used Google Tranalste API to make them into all English based comment for better sentiment analysis accuracy and add one extra column 'comment translated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator  \n",
    "for i in trange(comment_data1['comment'].count()):\n",
    "    text = comment_data1.loc[i, 'comment']\n",
    "    comment_data1.loc[i, 'comment_translated']=GoogleTranslator(source='auto', target='en').translate(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e60318",
   "metadata": {},
   "source": [
    "##### 8. Apply sentiment analysis using VADER on comments translated and perform data transformation to return the sentiment label of each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cda65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "comment_data1['scores'] = comment_data1['comment_translated'].apply(lambda comment_translated: sid.polarity_scores(str(comment_translated)))\n",
    "comment_data1\n",
    "\n",
    "comment_data1['compound'] = comment_data1['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "comment_data1\n",
    "\n",
    "def x(row):\n",
    "    if row.compound > 0:\n",
    "        return 'positive'\n",
    "    elif row.compound < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "comment_data1['sentiment_label'] = comment_data1.apply(x, axis=1)\n",
    "comment_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5691435",
   "metadata": {},
   "source": [
    "#####  9. Utilize NLTK for sentiment-based filtering of comments, removing stop words, and tokenizing documents. Additionally, eliminate emojis from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b11420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "positive_comment=comment_data1[(comment_data1['sentiment_label']=='positive')].reset_index(drop=True)\n",
    "positive_comment\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "vocab_list_F=[]\n",
    "for x in trange(0, positive_comment['comment_translated'].count()):\n",
    "    text = str(positive_comment.loc[x, \"comment_translated\"])\n",
    "    text = remove_emojis(text)\n",
    "    stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "    vocab_list=[i for i in word_tokenize(text.lower()) if i not in stop]\n",
    "    vocab_list_F.append(vocab_list)\n",
    "    \n",
    "flat_list = []\n",
    "\n",
    "for xs in vocab_list_F:\n",
    "    for x in xs:\n",
    "        flat_list.append(x)\n",
    "        \n",
    "flat_list\n",
    "\n",
    "#Turn list into dataframe\n",
    "positive_word_count = pd.DataFrame(flat_list, columns =['Vocabulary'])\n",
    "\n",
    "#Group by word count in descending order\n",
    "positive_word_count=negative_word_count.groupby('Vocabulary').size().sort_values(ascending=False).reset_index(name='word_count')\n",
    "positive_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a1c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_comment=comment_data1[(comment_data1['sentiment_label']=='negative')].reset_index(drop=True)\n",
    "negative_comment\n",
    "\n",
    "\n",
    "def remove_emojis(data):\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    return re.sub(emoj, '', data)\n",
    "\n",
    "vocab_list_F=[]\n",
    "for x in trange(0, positive_comment['comment_translated'].count()):\n",
    "    text = str(positive_comment.loc[x, \"comment_translated\"])\n",
    "    text = remove_emojis(text)\n",
    "    stop = set(stopwords.words('english') + list(string.punctuation))\n",
    "    vocab_list=[i for i in word_tokenize(text.lower()) if i not in stop]\n",
    "    vocab_list_F.append(vocab_list)\n",
    "    \n",
    "flat_list = []\n",
    "\n",
    "for xs in vocab_list_F:\n",
    "    for x in xs:\n",
    "        flat_list.append(x)\n",
    "        \n",
    "flat_list\n",
    "\n",
    "#Turn list into dataframe\n",
    "negative_comment_word_count = pd.DataFrame(flat_list, columns =['Vocabulary'])\n",
    "\n",
    "#Group by word count in descending order\n",
    "negative_comment_word_count=negative_word_count.groupby('Vocabulary').size().sort_values(ascending=False).reset_index(name='word_count')\n",
    "negative_comment_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75166d",
   "metadata": {},
   "source": [
    "##### 10. Extract every single unique title and concatenate them into string for more video category classification using Chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ce25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['video_Title']=df['video_Title'].str.replace('\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f','')\n",
    "\n",
    "video_list=df['video_Title'].unique().tolist()\n",
    "\n",
    "#concatenate unique video title into string\n",
    "video_string=','.join(video_list)\n",
    "video_list\n",
    "\n",
    "#concatenate unique video title into dataframe\n",
    "df1 = pd.DataFrame(video_list, columns =['video_Title'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db61d8e",
   "metadata": {},
   "source": [
    "##### 11. Utilized Open AI API to classify video into video categories and merge them back to the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b421016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import time\n",
    "from time import sleep\n",
    "\n",
    "openai.api_key = 'Your API key'\n",
    "\n",
    "def analyze_gpt35(text):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"\"\"You are trained to classify the given video title from Qatar Airways official \n",
    "        YouTube channel into one of the 7 following different categories based on the video title. 1.Scenic Landings and\n",
    "        Take-offs 2.Travel Adventures and Destinations 3.Qatar Airways Milestones and Achievements 4.Cultural and Holiday Celebrations \n",
    "        5.Behind-the-Scenes and Educational Content 6.Corporate Responsibility and Sustainability \n",
    "        7.Sports and Football Partnerships. For example: Video Title \"Digital Menus\" belongs to \"Corporate Responsibility and Sustainability\".\n",
    "        \"ITM 2021 – Chief Commercial Officer, Thierry Antinori\" belongs to \"Qatar Airways Milestones and Achievements\". If you are unsure\n",
    "        about the answer, classify them as \"Corporate Responsibility and Sustainability\" \"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Analyze the given video title and return answer with above given category only. For example, if the answer\n",
    "        is \"Beautiful Landings and Take-offs\", please then return \"Beautiful Landings and Take-offs\" only. Do not include any other words in the \n",
    "        answer. This is very important for me, please take more time to consider. You can make it!: {text}\"\"\"}\n",
    "        ]\n",
    "   \n",
    "    response = openai.ChatCompletion.create(\n",
    "                      model=\"gpt-3.5-turbo\",\n",
    "                      messages=messages, \n",
    "                      max_tokens=1, \n",
    "                      n=1, \n",
    "                      stop=None, \n",
    "                      temperature=0)\n",
    "\n",
    "    response_text = response.choices[0].message.content.strip().lower()\n",
    "\n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83fed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(326,youtubedata['text'].count()):\n",
    "    \n",
    "    if i % 3==0:\n",
    "        time.sleep(70)\n",
    "        text = df1.loc[i, 'text']\n",
    "        df1.loc[i, 'category']=analyze_gpt35(text)\n",
    "    else:\n",
    "        text = youtubedata.loc[i, 'text']\n",
    "        df1.loc[i, 'category']=analyze_gpt35(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.merge(comment_data1, how='left', on='video_Title')\n",
    "df2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
