{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e3d390",
   "metadata": {},
   "source": [
    "##### 1. Scrape all the playlist id from the channel and turn them into a dataframe called 'playlist_id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de46d781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient.discovery\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import html.parser as htmlparser\n",
    "import html\n",
    "import time\n",
    "import datetime as dt\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "DEVELOPER_KEY = \"AIzaSyC4QDfqJ_0tlokuQXEFdXWhUDckYa2_gDU\"\n",
    "\n",
    "youtube = googleapiclient.discovery.build(\n",
    "    api_service_name, api_version, developerKey=DEVELOPER_KEY)\n",
    "\n",
    "request2 = youtube.playlists().list(\n",
    "        fields = 'items/snippet,items/id',\n",
    "        channelId = 'UCi8xUU_lg3zr8UcBXLdEheQ',\n",
    "        maxResults = 50,\n",
    "        part = 'snippet'\n",
    ")\n",
    "\n",
    "platlists = []\n",
    "\n",
    "response2 = request2.execute()\n",
    "\n",
    "for item2 in response2['items']:\n",
    "    playlist = item2['snippet']\n",
    "    playlist_id=item2['id']\n",
    "    platlists.append([\n",
    "    playlist_id,    \n",
    "    playlist['title']])\n",
    "    \n",
    "playlist_id = pd.DataFrame(platlists, columns=['playlist_id','playlist_title'])\n",
    "playlist_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b960db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save all playlist id into a list.\n",
    "play_list=playlist_id.playlist_id.to_list()\n",
    "play_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b45a8",
   "metadata": {},
   "source": [
    "##### 2. Scrape every video id from all the playlist and turn them into a dataframe called 'video_id_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a28dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id_data = pd.DataFrame() \n",
    "for i in play_list:\n",
    "    try:\n",
    "        request3 = youtube.playlistItems().list(\n",
    "                fields = 'nextPageToken,items/snippet/resourceId,items/snippet/title',\n",
    "                playlistId = i,\n",
    "                maxResults = 50,\n",
    "                part = 'snippet'\n",
    "        )\n",
    "\n",
    "        videos = []\n",
    "\n",
    "\n",
    "        response3 = request3.execute()\n",
    "\n",
    "        for item3 in response3['items']:\n",
    "            video = item3['snippet']\n",
    "            videos.append([str(i),\n",
    "            video['resourceId']['videoId'],    \n",
    "            video['title']])\n",
    "\n",
    "        video_id = pd.DataFrame(videos, columns=['playlist_id','videoId','video_Title'])\n",
    "        video_id_data = pd.concat([video_id_data, video_id], ignore_index=True)\n",
    "    except:\n",
    "        print(f'fail to scrape comment from playlist_id:{i}')\n",
    "        time.sleep(5)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1316918",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save every video id into a list\n",
    "video_list=video_id_data.videoId.to_list()\n",
    "video_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69858fed",
   "metadata": {},
   "source": [
    "##### 3. Scrape every top-level comment from every video id and turned into into dataframe called 'comment_data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "comment_data = pd.DataFrame() \n",
    "\n",
    "for i in video_list:\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "        part = 'snippet,replies', \n",
    "        videoId = i\n",
    "    )\n",
    "\n",
    "        comments = []\n",
    "\n",
    "        while request:\n",
    "\n",
    "            response = request.execute()\n",
    "            for item in response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append([\n",
    "                comment['videoId'],    \n",
    "                comment['authorDisplayName'],\n",
    "                comment['publishedAt'],\n",
    "                comment['updatedAt'],\n",
    "                comment['likeCount'],\n",
    "                html.unescape(comment['textDisplay']),\n",
    "                item['snippet']['totalReplyCount']\n",
    "                            ])\n",
    "                request = youtube.commentThreads().list_next(\n",
    "                request, response)\n",
    "                youtubedata_data = pd.DataFrame(comments, columns=['videoId','author', 'published_at', 'updated_at', 'like_count', 'comment','Reply_count'])   \n",
    "        comment_data = pd.concat([comment_data, youtubedata_data], ignore_index=True)\n",
    "    except:\n",
    "        print(f'fail to scrape comment from video_id:{i}')\n",
    "        time.sleep(5)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604aec3",
   "metadata": {},
   "source": [
    "##### 4. Perform data cleaning to keep hashtag only instead of html embeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b2d213",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(comment_data1['comment'].count()):\n",
    "    st=comment_data1.loc[i,'comment']\n",
    "    comment_data1.loc[i,'comment']=re.sub('https.{0,500}','',st)\n",
    "    comment_data1.loc[i,'comment']=re.sub('<br>','',st)\n",
    "comment_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d502a7",
   "metadata": {},
   "source": [
    "##### 5. Check if there's any comment containing empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13040ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data1=comment_data1[(comment_data1.comment == '')].reset_index(drop=True)\n",
    "comment_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57af68ca",
   "metadata": {},
   "source": [
    "##### 6. Merging various dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_data1=comment_data1.merge(video_id_data, how='inner', on='videoId')\n",
    "comment_data1=comment_data1.merge(playlist_id, how='inner', on='playlist_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cec703",
   "metadata": {},
   "source": [
    "##### 7. Since some comments are not in English, I used Google Tranalste API to make them into all English based comment for better sentiment analysis accuracy and add one extra column 'comment translated'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8b6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trange(comment_data1['comment'].count()):\n",
    "    text = comment_data1.loc[i, 'comment']\n",
    "    comment_data1.loc[i, 'comment_translated']=GoogleTranslator(source='auto', target='en').translate(text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc54947",
   "metadata": {},
   "source": [
    "##### 8. Apply sentiment analysis using VADER on comments translated and perform data transformation to return the sentiment label of each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cda65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "comment_data1['scores'] = comment_data1['comment_translated'].apply(lambda comment_translated: sid.polarity_scores(str(comment_translated)))\n",
    "comment_data1\n",
    "\n",
    "comment_data1['compound'] = comment_data1['scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "comment_data1\n",
    "\n",
    "def x(row):\n",
    "    if row.compound > 0:\n",
    "        return 'positive'\n",
    "    elif row.compound < 0:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    \n",
    "comment_data1['sentiment_label'] = comment_data1.apply(x, axis=1)\n",
    "comment_data1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
